{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8702492d",
   "metadata": {},
   "source": [
    "## 사이킷런의 정밀도와 재현율\n",
    "\n",
    "- 의학(재현율) : Positive -> negative (큰일남)\n",
    "- 스팸매일(정밀도)\n",
    "- 재현율과 정밀도는 상보관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168811db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version -  1.20.3\n",
      "pandas version - 1.3.4\n",
      "sklearn version -  0.24.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('numpy version - ',np.__version__)\n",
    "print('pandas version -',pd.__version__)\n",
    "# ml\n",
    "import sklearn\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.datasets import load_iris,load_breast_cancer\n",
    "\n",
    "print('sklearn version - ',sklearn.__version__)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold,cross_val_score,\\\n",
    "cross_validate,GridSearchCV,train_test_split \n",
    "\n",
    "from sklearn.tree            import DecisionTreeClassifier \n",
    "from sklearn.linear_model    import LogisticRegression #\n",
    "from sklearn.ensemble        import RandomForestClassifier #\n",
    "from sklearn.metrics         import accuracy_score, recall_score,\\\n",
    "precision_score, f1_score, confusion_matrix, make_scorer,\\\n",
    "precision_recall_curve\n",
    "                            #(2진분류에만 사용)\n",
    "from sklearn.impute          import SimpleImputer\n",
    "\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "from sklearn.preprocessing   import LabelEncoder, OneHotEncoder,\\\n",
    "StandardScaler,MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7653bbf",
   "metadata": {},
   "source": [
    "#### 분류모델의 성능평가\n",
    "- 정확도 : 실 데이터와 예측 데이터가 얼마나 같은지를 판단하는 지표\n",
    "- 문제점? - 이진분류의 경우 모델의 성능을 왜곡할 수 있다.\n",
    "- why? : 데이터의 불균형\n",
    "- 해결책 : F1 Score(Prescision, Recall)(이진분류의 데이터 불균형인 경우)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa036a71",
   "metadata": {},
   "source": [
    "#### 분류모델 성능 평가를 위한 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa5a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP(target = 1, predict = 1)\n",
      "TN(target = 0, predict = 0)\n",
      "FN(target = 1, predict = 0) -> type 2 error\n",
      "FP(target = 0, predict = 1) -> type 1 error\n",
      "\n",
      "\n",
      "TP(target = 1, predict = 1)  3\n",
      "TN(target = 0, predict = 0)  0\n",
      "FN(target = 1, predict = 0) -> type 2 error  4\n",
      "FP(target = 0, predict = 1) -> type 1 error  3\n"
     ]
    }
   ],
   "source": [
    "print('TP(target = 1, predict = 1)')\n",
    "print('TN(target = 0, predict = 0)')\n",
    "print('FN(target = 1, predict = 0) -> type 2 error')\n",
    "print('FP(target = 0, predict = 1) -> type 1 error')\n",
    "\n",
    "target = [1,0,0,1,1,1,0,1,1,1]\n",
    "prediction = [0,1,1,1,1,0,1,0,1,0]\n",
    "\n",
    "tp = tn = fn = fp = 0\n",
    "\n",
    "for idx in range(len(target)):\n",
    "    #TP\n",
    "    if target[idx] == 1 and prediction[idx] == 1 :\n",
    "        tp += 1\n",
    "    #TN\n",
    "    if target[idx] == 0 and prediction[idx] == 0 :\n",
    "        tn += 1\n",
    "    #FN\n",
    "    if target[idx] == 1 and prediction[idx] == 0 :\n",
    "        fn += 1\n",
    "    #FP\n",
    "    if target[idx] == 0 and prediction[idx] == 1 :\n",
    "        fp += 1\n",
    "        \n",
    "print()\n",
    "print()\n",
    "print('TP(target = 1, predict = 1) ', tp)\n",
    "print('TN(target = 0, predict = 0) ', tn)\n",
    "print('FN(target = 1, predict = 0) -> type 2 error ',fn)\n",
    "print('FP(target = 0, predict = 1) -> type 1 error ',fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af231949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy -  0.3\n",
      "recall -  0.42857142857142855\n",
      "precision -  0.5\n",
      "f1 score -  0.4615384615384615\n",
      "\n",
      "confusion_matrix - \n",
      "  [[0 3]\n",
      " [4 3]]\n"
     ]
    }
   ],
   "source": [
    "print('accurancy - ',accuracy_score(target,prediction))\n",
    "# 2진 분류 불균형 데이터 모델인 경우\n",
    "print('recall - ', recall_score(target,prediction))\n",
    "print('precision - ', precision_score(target,prediction))\n",
    "print('f1 score - ',f1_score(target,prediction)) # Harmonic mean\n",
    "print()\n",
    "print('confusion_matrix - \\n ',confusion_matrix(target, prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79210632",
   "metadata": {},
   "source": [
    "- 정밀도(Precision) : TP / (FP + TP)\n",
    "- 상대적으로 정밀도가 더 중요한 지표인 경우의 모델? - 스팸메일\n",
    "- 재현율(Recall) : TP / (FN + TP)\n",
    "- 상대적으로 재현율 더 중요한 지표인 경우의 모델? - 의학(암진단), 금융(사기판별)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a05965d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv('C:/Users/shhmu/data/titanic_train.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7890ce26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0382952e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "886    0\n",
       "887    1\n",
       "888    0\n",
       "889    1\n",
       "890    0\n",
       "Name: Survived, Length: 891, dtype: int64>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['Survived'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75493268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. target, feature 로 데이터 분리\n",
      "target_type -  <class 'pandas.core.series.Series'>\n",
      "feature_type -  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
       "       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('2. target, feature 로 데이터 분리')\n",
    "titanic = pd.read_csv('C:/Users/shhmu/data/titanic_train.csv')\n",
    "titanic_target = titanic['Survived']\n",
    "titanic_feature = titanic.drop(['Survived'], axis =1)\n",
    "\n",
    "print('target_type - ', type(titanic_target))\n",
    "print('feature_type - ', type(titanic_feature))\n",
    "titanic_feature.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1f93d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      0\n",
       "      ..\n",
       "886    0\n",
       "887    1\n",
       "888    0\n",
       "889    1\n",
       "890    0\n",
       "Name: Survived, Length: 891, dtype: int64>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_target.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719d8639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. 전처리 요구사항 - \n",
      "불필요한 피처 제거 - PassengerId,Name,Ticket\n",
      "결측값 처리 - Age는 평균, Cabin는 N(label 인코딩(종류가 너무 많아)),Embarked는 N\n",
      "레이블 인코딩 - Sex, Cabin, Embarked\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                               Name  \\\n",
       "0              1       3                            Braund, Mr. Owen Harris   \n",
       "1              2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2              3       3                             Heikkinen, Miss. Laina   \n",
       "3              4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4              5       3                           Allen, Mr. William Henry   \n",
       "..           ...     ...                                                ...   \n",
       "886          887       2                              Montvila, Rev. Juozas   \n",
       "887          888       1                       Graham, Miss. Margaret Edith   \n",
       "888          889       3           Johnston, Miss. Catherine Helen \"Carrie\"   \n",
       "889          890       1                              Behr, Mr. Karl Howell   \n",
       "890          891       3                                Dooley, Mr. Patrick   \n",
       "\n",
       "        Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "0      male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1    female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2    female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3    female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4      male  35.0      0      0            373450   8.0500   NaN        S  \n",
       "..      ...   ...    ...    ...               ...      ...   ...      ...  \n",
       "886    male  27.0      0      0            211536  13.0000   NaN        S  \n",
       "887  female  19.0      0      0            112053  30.0000   B42        S  \n",
       "888  female   NaN      1      2        W./C. 6607  23.4500   NaN        S  \n",
       "889    male  26.0      0      0            111369  30.0000  C148        C  \n",
       "890    male  32.0      0      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('3. 전처리 요구사항 - ')\n",
    "print('불필요한 피처 제거 - PassengerId,Name,Ticket')\n",
    "print('결측값 처리 - Age는 평균, Cabin는 N(label 인코딩(종류가 너무 많아)),\\\n",
    "Embarked는 N')\n",
    "print('레이블 인코딩 - Sex, Cabin, Embarked')\n",
    "titanic = pd.read_csv('C:/Users/shhmu/data/titanic_train.csv')\n",
    "titanic_c = titanic.copy()\n",
    "titanic_target = titanic_c['Survived']\n",
    "titanic_feature = titanic_c.drop(['Survived'], axis =1)\n",
    "titanic_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04d330ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_feature(frm):\n",
    "    frm.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "    return frm\n",
    "\n",
    "def pre_processing(frm):\n",
    "    frm['Age'].fillna(frm['Age'].mean(), inplace = True)\n",
    "    frm['Cabin'].fillna('N',inplace=True)\n",
    "    frm['Embarked'].fillna('N',inplace=True)\n",
    "    return frm\n",
    "\n",
    "def label_encoder(frm):\n",
    "    frm['Cabin'] = frm['Cabin'].str[:1] #슬라이싱 가능\n",
    "    features = ['Sex','Cabin','Embarked']\n",
    "    for feature in features:\n",
    "        encoder = LabelEncoder()\n",
    "        frm[feature] = encoder.fit_transform(frm[feature])\n",
    "    \n",
    "    return frm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99a22410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex        Age  SibSp  Parch     Fare  Cabin  Embarked\n",
       "0         3    1  22.000000      1      0   7.2500      7         3\n",
       "1         1    0  38.000000      1      0  71.2833      2         0\n",
       "2         3    0  26.000000      0      0   7.9250      7         3\n",
       "3         1    0  35.000000      1      0  53.1000      2         3\n",
       "4         3    1  35.000000      0      0   8.0500      7         3\n",
       "..      ...  ...        ...    ...    ...      ...    ...       ...\n",
       "886       2    1  27.000000      0      0  13.0000      7         3\n",
       "887       1    0  19.000000      0      0  30.0000      1         3\n",
       "888       3    0  29.699118      1      2  23.4500      7         3\n",
       "889       1    1  26.000000      0      0  30.0000      2         0\n",
       "890       3    1  32.000000      0      0   7.7500      7         2\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv('C:/Users/shhmu/data/titanic_train.csv')\n",
    "titanic_c = titanic.copy()\n",
    "titanic_target = titanic_c['Survived']\n",
    "titanic_feature = titanic_c.drop(['Survived'], axis =1)\n",
    "\n",
    "feature_subset = drop_feature(titanic_feature)\n",
    "feature_subset = pre_processing(feature_subset)\n",
    "feature_subset = label_encoder(feature_subset)\n",
    "feature_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "386faa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습과 테스트 분리 - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('학습과 테스트 분리 - ')\n",
    "print()\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_subset,\n",
    "                                                   titanic_target,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f4ee33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 8), (179, 8), (712,), (179,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdee68de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "msno.matrix(feature_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd113783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(x_train,y_train)\n",
    "lr_y_pred = lr_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b64de80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(x_train,y_train)\n",
    "dt_y_pred = lr_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "777a982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_eval(target, prediction):\n",
    "    print('accurancy - ',accuracy_score(target,prediction))\n",
    "    print('recall - ', recall_score(target,prediction))\n",
    "    print('precision - ', precision_score(target,prediction))\n",
    "    print('f1 score - ',f1_score(target,prediction)) # Harmonic mean\n",
    "    print()\n",
    "    print('confusion_matrix - \\n ',confusion_matrix(target, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "810329b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression - \n",
      "\n",
      "accurancy -  0.770949720670391\n",
      "recall -  0.7066666666666667\n",
      "precision -  0.7361111111111112\n",
      "f1 score -  0.7210884353741497\n",
      "\n",
      "confusion_matrix - \n",
      "  [[85 19]\n",
      " [22 53]]\n",
      "accurancy -  0.770949720670391\n"
     ]
    }
   ],
   "source": [
    "print('LogisticRegression - ')\n",
    "print()\n",
    "metrics_eval(y_test, lr_y_pred) #metrics [[F,T],[F,T]]\n",
    "print('accurancy - ',(85+53)/(85+19+22+53))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e1d56e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier - \n",
      "\n",
      "accurancy -  0.770949720670391\n",
      "recall -  0.7066666666666667\n",
      "precision -  0.7361111111111112\n",
      "f1 score -  0.7210884353741497\n",
      "\n",
      "confusion_matrix - \n",
      "  [[85 19]\n",
      " [22 53]]\n"
     ]
    }
   ],
   "source": [
    "print('DecisionTreeClassifier - ')\n",
    "print()\n",
    "metrics_eval(y_test, dt_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c2977bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차검증 - \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['fit_time', 'score_time', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('교차검증 - ')\n",
    "fold = KFold(n_splits=20)\n",
    "scoring ={\n",
    "    'accuracy' : make_scorer(accuracy_score),\n",
    "    'precision' : make_scorer(precision_score),\n",
    "    'recall' : make_scorer(recall_score),\n",
    "    'f1' : make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "result = cross_validate(lr_model, x_train, y_train,\n",
    "                       cv = fold,\n",
    "                       scoring = scoring)\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65b3ff00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7990476190476192"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['test_accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a234ef50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer = load_breast_cancer()\n",
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e34aa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유방암 관련 데이터 - 정확도,재현율(실제 P를 N 예측하면 안되기 때문에)재현율을 볼것\n",
      "재현율은 실제 양성을 양성으로 예측한 비율이 높아야 성능이 좋은 모델\n"
     ]
    }
   ],
   "source": [
    "print('유방암 관련 데이터 - 정확도,재현율(실제 P를 N 예측하면 안되기 때문에)\\\n",
    "재현율을 볼것')\n",
    "print('재현율은 실제 양성을 양성으로 예측한 비율이 높아야 성능이 좋은 모델')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cea0386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 데이터 프레임 만들기(피처, 타겟)포함\n",
    "cancer_df = pd.DataFrame(cancer.data, columns = cancer.feature_names)\n",
    "cancer_df['target'] = cancer.target\n",
    "cancer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bad8eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 타겟에 대한 균형 여부를 확인\n",
    "cancer_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72ced03a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method IndexOpsMixin.value_counts of 0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "564    0\n",
       "565    0\n",
       "566    0\n",
       "567    0\n",
       "568    1\n",
       "Name: target, Length: 569, dtype: int32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_df['target'].value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75015bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_target = cancer_df['target']\n",
    "cancer_feature = cancer_df.drop(['target'], axis = 1)\n",
    "cancer_feature # 변수에 저장하면 drop([타겟],inplace=True)없어도 타겟 없어짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1db2d469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_df = pd.DataFrame(cancer.data, columns = cancer.feature_names)\n",
    "cancer_df['target'] = cancer.target\n",
    "cancer_df.drop(['target'], axis = 1)\n",
    "cancer_df # 그냥 drop만 사용하면 변경된 내용이 저장되지않아 위의 타겟이 그대로\n",
    "############ 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3daa5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(cancer_feature,\n",
    "                                                   cancer_target,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f26c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RandomForestClassifier 모델\n",
    "cancer_rfc_model = RandomForestClassifier()\n",
    "cancer_rfc_model.fit(x_train,y_train)\n",
    "rfc_y_pred = cancer_rfc_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d69b126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy -  0.9473684210526315\n",
      "recall -  0.9736842105263158\n",
      "precision -  0.9487179487179487\n",
      "f1 score -  0.9610389610389611\n",
      "\n",
      "confusion_matrix - \n",
      "  [[34  4]\n",
      " [ 2 74]]\n"
     ]
    }
   ],
   "source": [
    "# 5. 평가지표 확인\n",
    "metrics_eval(y_test, rfc_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6600bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "교차검증 - \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['fit_time', 'score_time', 'test_accuracy', 'test_precision', 'test_recall', 'test_f1'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('교차검증 - ')\n",
    "fold = KFold(n_splits=20)\n",
    "scoring ={\n",
    "    'accuracy' : make_scorer(accuracy_score),\n",
    "    'precision' : make_scorer(precision_score),\n",
    "    'recall' : make_scorer(recall_score),\n",
    "    'f1' : make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "result = cross_validate(cancer_rfc_model, x_train, y_train,\n",
    "                       cv = fold,\n",
    "                       scoring = scoring)\n",
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed056e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9667193223443222"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['test_recall'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c937377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9534584980237153"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['test_accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "867fe5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불필요한 피처 제거 - PassengerId,Name,Ticket\n",
    "titanic = pd.read_csv('C:/Users/shhmu/data/titanic_train.csv')\n",
    "titanic_target = titanic['Survived'].copy()\n",
    "titanic_feature = titanic.drop(['Survived'], axis =1 ).copy()\n",
    "titanic_feature.drop(['Name','Ticket','PassengerId'], axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b20531f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측값 처리 - Age는 평균, Cabin는 N(label 인코딩(종류가 너무 많아)),Embarked는 N\n"
     ]
    }
   ],
   "source": [
    "print('결측값 처리 - Age는 평균, Cabin는 N(label 인코딩(종류가 너무 많아)),\\\n",
    "Embarked는 N')\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "titanic_feature['Age'] = imputer.fit_transform(titanic_feature[['Age']])\n",
    "\n",
    "titanic_feature['Cabin'] = titanic_feature['Cabin'].fillna('N')\n",
    "titanic_feature['Embarked'] = titanic_feature['Embarked'].fillna('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63fd9bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블 인코딩 - Sex, Cabin, Embarked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shhmu\\AppData\\Local\\Temp/ipykernel_13092/4176060959.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titanic_feature['Cabin'][i] = 'C'\n",
      "C:\\Users\\shhmu\\AppData\\Local\\Temp/ipykernel_13092/4176060959.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titanic_feature['Cabin'][i] = 'E'\n",
      "C:\\Users\\shhmu\\AppData\\Local\\Temp/ipykernel_13092/4176060959.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titanic_feature['Cabin'][i] = 'G'\n",
      "C:\\Users\\shhmu\\AppData\\Local\\Temp/ipykernel_13092/4176060959.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titanic_feature['Cabin'][i] = 'D'\n",
      "C:\\Users\\shhmu\\AppData\\Local\\Temp/ipykernel_13092/4176060959.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titanic_feature['Cabin'][i] = 'A'\n",
      "C:\\Users\\shhmu\\AppData\\Local\\Temp/ipykernel_13092/4176060959.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titanic_feature['Cabin'][i] = 'B'\n",
      "C:\\Users\\shhmu\\AppData\\Local\\Temp/ipykernel_13092/4176060959.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titanic_feature['Cabin'][i] = 'F'\n"
     ]
    }
   ],
   "source": [
    "print('레이블 인코딩 - Sex, Cabin, Embarked')\n",
    "encoder = LabelEncoder()\n",
    "titanic_feature['Sex'] = encoder.fit(titanic_feature['Sex']).transform(titanic_feature['Sex'])\n",
    "titanic_feature['Embarked'] = encoder.fit(titanic_feature['Embarked']).transform(titanic_feature['Embarked'])\n",
    "for i,v in enumerate(titanic_feature['Cabin']):\n",
    "    if 'A' in v:\n",
    "        titanic_feature['Cabin'][i] = 'A'\n",
    "    elif 'B' in v:\n",
    "        titanic_feature['Cabin'][i] = 'B'\n",
    "    elif 'C' in v:\n",
    "        titanic_feature['Cabin'][i] = 'C'\n",
    "    elif 'D' in v:\n",
    "        titanic_feature['Cabin'][i] = 'D'\n",
    "    elif 'E' in v:\n",
    "        titanic_feature['Cabin'][i] = 'E'\n",
    "    elif 'F' in v:\n",
    "        titanic_feature['Cabin'][i] = 'F'\n",
    "    elif 'G' in v:\n",
    "        titanic_feature['Cabin'][i] = 'G'\n",
    "titanic_feature['Cabin'] = encoder.fit(titanic_feature['Cabin']).transform(titanic_feature['Cabin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c3446a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train, x1_test, y1_train, y1_test = train_test_split(titanic_feature,\n",
    "                                                   titanic_target,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state= 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be9605fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1_model = DecisionTreeClassifier()\n",
    "dt1_model.fit(x1_train,y1_train)\n",
    "dt1_y_pred = lr_model.predict(x1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cf3dd3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy -  0.770949720670391\n",
      "recall -  0.7066666666666667\n",
      "precision -  0.7361111111111112\n",
      "f1 score -  0.7210884353741497\n",
      "\n",
      "confusion_matrix - \n",
      "  [[85 19]\n",
      " [22 53]]\n"
     ]
    }
   ],
   "source": [
    "metrics_eval(y1_test, dt1_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c48d6cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "재현율을 높이기위한 방법으로 - GridSearchCV를 이용한 파라미터 튜닝!!\n",
      "n_estimators - tree 개수를 의미\n",
      "max_features - 최대 선택할 피처의 수를 의미\n",
      "max_depth - 최대 선택할 트리의 깊이를 의미\n"
     ]
    }
   ],
   "source": [
    "print('재현율을 높이기위한 방법으로 - GridSearchCV를 이용한 파라미터 튜닝!!')\n",
    "print('n_estimators - tree 개수를 의미')\n",
    "print('max_features - 최대 선택할 피처의 수를 의미')\n",
    "print('max_depth - 최대 선택할 트리의 깊이를 의미')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2a5b198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=20, estimator=RandomForestClassifier(),\n",
       "             param_grid={'max_depth': [4, 6, 8], 'max_features': [6, 8, 15, 20],\n",
       "                         'n_estimators': [50, 100, 150, 200]},\n",
       "             scoring='recall')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = {\n",
    "    'n_estimators' : [50, 100, 150, 200],\n",
    "    'max_features' : [6, 8, 15, 20],\n",
    "    'max_depth'    : [4, 6, 8] #뎁스가 깊다고 성능이 좋은 것은 아니다.\n",
    "}\n",
    "cancer_rfc_model = RandomForestClassifier()\n",
    "grid_search_model =GridSearchCV(cancer_rfc_model,\n",
    "                               param_grid = param,\n",
    "                               cv = 20,\n",
    "                               refit = True,\n",
    "                               scoring = 'recall')\n",
    "grid_search_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c29f5f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.11435127, 0.19296108, 0.2958046 , 0.37699218, 0.10919999,\n",
       "        0.20821631, 0.35138043, 0.4165308 , 0.13318487, 0.26677341,\n",
       "        0.37466059, 0.54073932, 0.13973107, 0.29734337, 0.4447969 ,\n",
       "        0.62245162, 0.10250232, 0.22636254, 0.30427059, 0.38232689,\n",
       "        0.10028259, 0.21713344, 0.29775587, 0.43005087, 0.13115267,\n",
       "        0.25698063, 0.41965671, 0.56164912, 0.16801935, 0.33702139,\n",
       "        0.46643128, 0.63868554, 0.09773208, 0.19819417, 0.26655184,\n",
       "        0.41008685, 0.11435164, 0.2076036 , 0.31381714, 0.41332023,\n",
       "        0.14331443, 0.25510495, 0.39696167, 0.52067407, 0.15986773,\n",
       "        0.30649664, 0.48416511, 0.60320566]),\n",
       " 'std_fit_time': array([0.04697096, 0.02293547, 0.02470187, 0.04060633, 0.02409843,\n",
       "        0.03878102, 0.06333633, 0.03085557, 0.01411586, 0.0319111 ,\n",
       "        0.04254489, 0.03273574, 0.0162079 , 0.05705993, 0.05220062,\n",
       "        0.04727954, 0.01506028, 0.05683911, 0.03094067, 0.03722611,\n",
       "        0.00963647, 0.03222445, 0.01782219, 0.06347251, 0.0124376 ,\n",
       "        0.02746417, 0.06292157, 0.07985214, 0.02344282, 0.05449508,\n",
       "        0.03219387, 0.0400541 , 0.01484159, 0.02874893, 0.02166191,\n",
       "        0.08080419, 0.02304437, 0.01912129, 0.04237957, 0.03538124,\n",
       "        0.0285662 , 0.02153612, 0.02995225, 0.03568356, 0.02784177,\n",
       "        0.04281716, 0.07699733, 0.07559135]),\n",
       " 'mean_score_time': array([0.00966913, 0.01462263, 0.01821138, 0.02150599, 0.00951035,\n",
       "        0.01244535, 0.01871508, 0.02315404, 0.00857552, 0.01240103,\n",
       "        0.01771522, 0.02238681, 0.00673894, 0.01196965, 0.01588757,\n",
       "        0.02315106, 0.00847722, 0.01539582, 0.01779199, 0.02292904,\n",
       "        0.00716772, 0.01332647, 0.01627644, 0.02526726, 0.00745623,\n",
       "        0.0114593 , 0.01929327, 0.02502929, 0.00801375, 0.01340805,\n",
       "        0.01733836, 0.02136514, 0.00828197, 0.01233354, 0.0168927 ,\n",
       "        0.02308527, 0.00797374, 0.01285461, 0.01854365, 0.02330415,\n",
       "        0.00797697, 0.01146585, 0.01743987, 0.021984  , 0.00738491,\n",
       "        0.01191015, 0.01855322, 0.02126473]),\n",
       " 'std_score_time': array([0.00539144, 0.00433609, 0.00397048, 0.00241269, 0.00597583,\n",
       "        0.00211106, 0.0043866 , 0.00398378, 0.00194172, 0.00233993,\n",
       "        0.00592464, 0.00303783, 0.00069881, 0.0020235 , 0.00177221,\n",
       "        0.00440029, 0.00249873, 0.00639817, 0.00246295, 0.00566467,\n",
       "        0.00086343, 0.00441977, 0.00232779, 0.00953768, 0.00102333,\n",
       "        0.00215527, 0.00644139, 0.00929482, 0.00229332, 0.00404629,\n",
       "        0.00267861, 0.00224757, 0.00267934, 0.00174619, 0.00362287,\n",
       "        0.00547298, 0.00170034, 0.00231489, 0.00530865, 0.0039447 ,\n",
       "        0.00207009, 0.00159574, 0.00422521, 0.00410895, 0.00145984,\n",
       "        0.00310809, 0.00419524, 0.00367372]),\n",
       " 'param_max_depth': masked_array(data=[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=[6, 6, 6, 6, 8, 8, 8, 8, 15, 15, 15, 15, 20, 20, 20, 20,\n",
       "                    6, 6, 6, 6, 8, 8, 8, 8, 15, 15, 15, 15, 20, 20, 20, 20,\n",
       "                    6, 6, 6, 6, 8, 8, 8, 8, 15, 15, 15, 15, 20, 20, 20, 20],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[50, 100, 150, 200, 50, 100, 150, 200, 50, 100, 150,\n",
       "                    200, 50, 100, 150, 200, 50, 100, 150, 200, 50, 100,\n",
       "                    150, 200, 50, 100, 150, 200, 50, 100, 150, 200, 50,\n",
       "                    100, 150, 200, 50, 100, 150, 200, 50, 100, 150, 200,\n",
       "                    50, 100, 150, 200],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 4, 'max_features': 6, 'n_estimators': 50},\n",
       "  {'max_depth': 4, 'max_features': 6, 'n_estimators': 100},\n",
       "  {'max_depth': 4, 'max_features': 6, 'n_estimators': 150},\n",
       "  {'max_depth': 4, 'max_features': 6, 'n_estimators': 200},\n",
       "  {'max_depth': 4, 'max_features': 8, 'n_estimators': 50},\n",
       "  {'max_depth': 4, 'max_features': 8, 'n_estimators': 100},\n",
       "  {'max_depth': 4, 'max_features': 8, 'n_estimators': 150},\n",
       "  {'max_depth': 4, 'max_features': 8, 'n_estimators': 200},\n",
       "  {'max_depth': 4, 'max_features': 15, 'n_estimators': 50},\n",
       "  {'max_depth': 4, 'max_features': 15, 'n_estimators': 100},\n",
       "  {'max_depth': 4, 'max_features': 15, 'n_estimators': 150},\n",
       "  {'max_depth': 4, 'max_features': 15, 'n_estimators': 200},\n",
       "  {'max_depth': 4, 'max_features': 20, 'n_estimators': 50},\n",
       "  {'max_depth': 4, 'max_features': 20, 'n_estimators': 100},\n",
       "  {'max_depth': 4, 'max_features': 20, 'n_estimators': 150},\n",
       "  {'max_depth': 4, 'max_features': 20, 'n_estimators': 200},\n",
       "  {'max_depth': 6, 'max_features': 6, 'n_estimators': 50},\n",
       "  {'max_depth': 6, 'max_features': 6, 'n_estimators': 100},\n",
       "  {'max_depth': 6, 'max_features': 6, 'n_estimators': 150},\n",
       "  {'max_depth': 6, 'max_features': 6, 'n_estimators': 200},\n",
       "  {'max_depth': 6, 'max_features': 8, 'n_estimators': 50},\n",
       "  {'max_depth': 6, 'max_features': 8, 'n_estimators': 100},\n",
       "  {'max_depth': 6, 'max_features': 8, 'n_estimators': 150},\n",
       "  {'max_depth': 6, 'max_features': 8, 'n_estimators': 200},\n",
       "  {'max_depth': 6, 'max_features': 15, 'n_estimators': 50},\n",
       "  {'max_depth': 6, 'max_features': 15, 'n_estimators': 100},\n",
       "  {'max_depth': 6, 'max_features': 15, 'n_estimators': 150},\n",
       "  {'max_depth': 6, 'max_features': 15, 'n_estimators': 200},\n",
       "  {'max_depth': 6, 'max_features': 20, 'n_estimators': 50},\n",
       "  {'max_depth': 6, 'max_features': 20, 'n_estimators': 100},\n",
       "  {'max_depth': 6, 'max_features': 20, 'n_estimators': 150},\n",
       "  {'max_depth': 6, 'max_features': 20, 'n_estimators': 200},\n",
       "  {'max_depth': 8, 'max_features': 6, 'n_estimators': 50},\n",
       "  {'max_depth': 8, 'max_features': 6, 'n_estimators': 100},\n",
       "  {'max_depth': 8, 'max_features': 6, 'n_estimators': 150},\n",
       "  {'max_depth': 8, 'max_features': 6, 'n_estimators': 200},\n",
       "  {'max_depth': 8, 'max_features': 8, 'n_estimators': 50},\n",
       "  {'max_depth': 8, 'max_features': 8, 'n_estimators': 100},\n",
       "  {'max_depth': 8, 'max_features': 8, 'n_estimators': 150},\n",
       "  {'max_depth': 8, 'max_features': 8, 'n_estimators': 200},\n",
       "  {'max_depth': 8, 'max_features': 15, 'n_estimators': 50},\n",
       "  {'max_depth': 8, 'max_features': 15, 'n_estimators': 100},\n",
       "  {'max_depth': 8, 'max_features': 15, 'n_estimators': 150},\n",
       "  {'max_depth': 8, 'max_features': 15, 'n_estimators': 200},\n",
       "  {'max_depth': 8, 'max_features': 20, 'n_estimators': 50},\n",
       "  {'max_depth': 8, 'max_features': 20, 'n_estimators': 100},\n",
       "  {'max_depth': 8, 'max_features': 20, 'n_estimators': 150},\n",
       "  {'max_depth': 8, 'max_features': 20, 'n_estimators': 200}],\n",
       " 'split0_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split1_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split2_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split3_test_score': array([0.92857143, 0.92857143, 0.92857143, 0.92857143, 1.        ,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 1.        , 0.92857143, 0.92857143,\n",
       "        1.        , 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        1.        , 1.        , 1.        , 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 1.        , 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        1.        , 0.92857143, 0.92857143]),\n",
       " 'split4_test_score': array([0.92857143, 0.92857143, 1.        , 1.        , 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 1.        , 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 1.        , 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 1.        ,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143]),\n",
       " 'split5_test_score': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.92857143, 0.92857143,\n",
       "        0.92857143, 1.        , 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.92857143, 1.        , 1.        , 1.        , 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.92857143, 1.        , 1.        , 1.        ,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        1.        , 0.92857143, 0.92857143]),\n",
       " 'split6_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split7_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split8_test_score': array([1.        , 1.        , 1.        , 1.        , 0.85714286,\n",
       "        1.        , 1.        , 1.        , 0.92857143, 0.92857143,\n",
       "        1.        , 0.85714286, 0.85714286, 0.92857143, 0.92857143,\n",
       "        0.85714286, 0.92857143, 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.92857143, 0.92857143, 1.        , 1.        ,\n",
       "        0.85714286, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 1.        , 0.92857143, 1.        ,\n",
       "        0.92857143, 0.92857143, 1.        , 1.        , 1.        ,\n",
       "        0.92857143, 0.92857143, 0.85714286, 0.92857143, 0.85714286,\n",
       "        0.92857143, 0.92857143, 0.92857143]),\n",
       " 'split9_test_score': array([1.        , 0.92857143, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        ]),\n",
       " 'split10_test_score': array([0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 1.        , 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143]),\n",
       " 'split11_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split12_test_score': array([0.92857143, 0.92857143, 0.92857143, 0.92857143, 1.        ,\n",
       "        1.        , 0.92857143, 0.92857143, 0.92857143, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        1.        , 0.92857143, 0.92857143, 0.92857143, 1.        ,\n",
       "        1.        , 0.92857143, 0.92857143, 0.92857143, 1.        ,\n",
       "        1.        , 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 1.        , 0.92857143, 0.92857143, 0.92857143,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.92857143, 1.        ]),\n",
       " 'split13_test_score': array([0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143]),\n",
       " 'split14_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split15_test_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'split16_test_score': array([0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        1.        , 0.92857143, 0.92857143, 0.92857143, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 1.        , 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.92857143, 0.92857143, 0.92857143,\n",
       "        1.        , 0.92857143, 0.92857143, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        ]),\n",
       " 'split17_test_score': array([0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.85714286,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.85714286, 0.92857143, 0.85714286, 0.92857143, 0.92857143,\n",
       "        0.85714286, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.85714286, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.85714286,\n",
       "        0.92857143, 0.92857143, 0.92857143]),\n",
       " 'split18_test_score': array([0.92857143, 1.        , 1.        , 1.        , 0.92857143,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        ]),\n",
       " 'split19_test_score': array([0.92857143, 1.        , 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        1.        , 0.92857143, 1.        , 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143, 0.92857143, 0.92857143,\n",
       "        0.92857143, 0.92857143, 0.92857143]),\n",
       " 'mean_test_score': array([0.96785714, 0.97142857, 0.975     , 0.975     , 0.96428571,\n",
       "        0.97857143, 0.97142857, 0.97142857, 0.96428571, 0.97142857,\n",
       "        0.97142857, 0.97142857, 0.96785714, 0.97142857, 0.97142857,\n",
       "        0.96785714, 0.975     , 0.97142857, 0.97142857, 0.97142857,\n",
       "        0.975     , 0.96785714, 0.97142857, 0.975     , 0.97142857,\n",
       "        0.96428571, 0.96785714, 0.96785714, 0.96785714, 0.97142857,\n",
       "        0.975     , 0.97142857, 0.975     , 0.96785714, 0.975     ,\n",
       "        0.97142857, 0.96785714, 0.975     , 0.975     , 0.975     ,\n",
       "        0.97142857, 0.97142857, 0.96785714, 0.97142857, 0.96428571,\n",
       "        0.97857143, 0.96785714, 0.97142857]),\n",
       " 'std_test_score': array([0.03553527, 0.03499271, 0.03406926, 0.03406926, 0.04791574,\n",
       "        0.03273268, 0.03499271, 0.03499271, 0.03571429, 0.03499271,\n",
       "        0.04164966, 0.04164966, 0.04778246, 0.03499271, 0.03499271,\n",
       "        0.04778246, 0.03406926, 0.03499271, 0.03499271, 0.03499271,\n",
       "        0.03406926, 0.04210652, 0.03499271, 0.03406926, 0.03499271,\n",
       "        0.04225771, 0.03553527, 0.03553527, 0.03553527, 0.03499271,\n",
       "        0.03406926, 0.03499271, 0.03406926, 0.03553527, 0.03406926,\n",
       "        0.03499271, 0.03553527, 0.03406926, 0.03406926, 0.03406926,\n",
       "        0.03499271, 0.03499271, 0.04210652, 0.03499271, 0.04791574,\n",
       "        0.03273268, 0.03553527, 0.03499271]),\n",
       " 'rank_test_score': array([44, 14,  3,  3, 45,  1, 28, 28, 47, 14, 14, 14, 34, 14, 14, 34,  3,\n",
       "        28, 28, 28,  3, 34, 14,  3, 28, 47, 37, 37, 37, 14,  3, 14,  3, 37,\n",
       "         3, 14, 37,  3,  3,  3, 14, 14, 37, 14, 45,  1, 37, 14])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c21b6db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'max_features': 8, 'n_estimators': 100}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a461596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9785714285714284"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c979af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_estimator = grid_search_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4bcf93ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accurancy -  0.9473684210526315\n",
      "recall -  0.9736842105263158\n",
      "precision -  0.9487179487179487\n",
      "f1 score -  0.9610389610389611\n",
      "\n",
      "confusion_matrix - \n",
      "  [[34  4]\n",
      " [ 2 74]]\n"
     ]
    }
   ],
   "source": [
    "best_y_pred = rf_estimator.predict(x_test)\n",
    "metrics_eval(y_test, best_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306d7ec",
   "metadata": {},
   "source": [
    "#### 정밀도와 재현율을 임의로 조절하는 모델을 생성해야하는 경우\n",
    "- 분류 임계값이 낮을수록 Positive를 예측할 학률이 높아져 재현율이 증가\n",
    "- predict_proba()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80b3faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [\n",
    "    [-1,-1,2],\n",
    "    [2,0,0],\n",
    "    [0,1.1,1.2]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3125359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "holder = Binarizer(threshold = 1.1) # 같은 값까지 0\n",
    "print(holder.fit_transform(matrix) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "55dbcc32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
       "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_frm = pd.read_csv('C:/Users/shhmu/data/titanic_train.csv')\n",
    "titanic_frm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b548184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_target = titanic_frm['Survived']\n",
    "titanic_feature = titanic_frm.drop(['Survived'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c9417c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_feature(frm):\n",
    "    frm.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n",
    "    return frm\n",
    "\n",
    "def pre_processing(frm):\n",
    "    frm['Age'].fillna(frm['Age'].mean(), inplace = True)\n",
    "    frm['Cabin'].fillna('N',inplace=True)\n",
    "    frm['Embarked'].fillna('N',inplace=True)\n",
    "    return frm\n",
    "\n",
    "def label_encoder(frm):\n",
    "    frm['Cabin'] = frm['Cabin'].str[:1]\n",
    "    features = ['Sex','Cabin','Embarked']\n",
    "    for feature in features:\n",
    "        encoder = LabelEncoder()\n",
    "        frm[feature] = encoder.fit_transform(frm[feature])\n",
    "    \n",
    "    return frm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7d6081d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex        Age  SibSp  Parch     Fare  Cabin  Embarked\n",
       "0         3    1  22.000000      1      0   7.2500      7         3\n",
       "1         1    0  38.000000      1      0  71.2833      2         0\n",
       "2         3    0  26.000000      0      0   7.9250      7         3\n",
       "3         1    0  35.000000      1      0  53.1000      2         3\n",
       "4         3    1  35.000000      0      0   8.0500      7         3\n",
       "..      ...  ...        ...    ...    ...      ...    ...       ...\n",
       "886       2    1  27.000000      0      0  13.0000      7         3\n",
       "887       1    0  19.000000      0      0  30.0000      1         3\n",
       "888       3    0  29.699118      1      2  23.4500      7         3\n",
       "889       1    1  26.000000      0      0  30.0000      2         0\n",
       "890       3    1  32.000000      0      0   7.7500      7         2\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_subset = drop_feature(titanic_feature)\n",
    "feature_subset = pre_processing(feature_subset)\n",
    "feature_subset = label_encoder(feature_subset)\n",
    "feature_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2fb942c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(feature_subset,\n",
    "                                                   titanic_target,\n",
    "                                                   test_size = 0.2,\n",
    "                                                   random_state= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dee13241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712, 8), (179, 8), (712,), (179,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7529116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(x_train, y_train)\n",
    "y_pred = logistic_model.predict(x_test) # 예측정답나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "80df5235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확률예측 값 - predict_proba()\n",
      "type -  <class 'numpy.ndarray'>\n",
      "shape -  (179, 2)\n",
      "type -  <class 'numpy.ndarray'>\n",
      "shape -  (179,)\n"
     ]
    }
   ],
   "source": [
    "print('확률예측 값 - predict_proba()')\n",
    "predict_proba_result = logistic_model.predict_proba(x_test) # 예측확률 나옴\n",
    "print('type - ',type(predict_proba_result))\n",
    "print('shape - ',predict_proba_result.shape)\n",
    "print('type - ', type(y_pred))\n",
    "print('shape - ',y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "04942c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16620951, 0.83379049],\n",
       "       [0.26645451, 0.73354549],\n",
       "       [0.8965249 , 0.1034751 ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_proba_result[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a46e0d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44c13387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16620951, 0.83379049, 1.        ],\n",
       "       [0.26645451, 0.73354549, 1.        ],\n",
       "       [0.8965249 , 0.1034751 , 0.        ],\n",
       "       [0.40388898, 0.59611102, 1.        ],\n",
       "       [0.86336574, 0.13663426, 0.        ],\n",
       "       [0.25824152, 0.74175848, 1.        ],\n",
       "       [0.8998477 , 0.1001523 , 0.        ],\n",
       "       [0.78226965, 0.21773035, 0.        ],\n",
       "       [0.05887811, 0.94112189, 1.        ],\n",
       "       [0.63210508, 0.36789492, 0.        ]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred_prob_concat = np.concatenate([predict_proba_result,y_pred.reshape(-1,1)],axis =1)\n",
    "pred_prob_concat[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5911b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_th = 0.3\n",
    "predict_proba_positive = predict_proba_result[:,1].reshape(-1,1)\n",
    "user_pred = Binarizer(threshold = user_th)\\\n",
    ".fit_transform(predict_proba_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94c5a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default - \n",
      "accurancy -  0.8044692737430168\n",
      "recall -  0.72\n",
      "precision -  0.7941176470588235\n",
      "f1 score -  0.7552447552447551\n",
      "\n",
      "confusion_matrix - \n",
      "  [[90 14]\n",
      " [21 54]]\n",
      "\n",
      "user th - 0.3\n",
      "accurancy -  0.7932960893854749\n",
      "recall -  0.7733333333333333\n",
      "precision -  0.7435897435897436\n",
      "f1 score -  0.7581699346405228\n",
      "\n",
      "confusion_matrix - \n",
      "  [[84 20]\n",
      " [17 58]]\n"
     ]
    }
   ],
   "source": [
    "print('default - ')\n",
    "metrics_eval(y_test, y_pred)\n",
    "print()\n",
    "print('user th - 0.3')\n",
    "metrics_eval(y_test,user_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b040f6d",
   "metadata": {},
   "source": [
    "#### trade-off 시각화\n",
    "- precision_recall_curve(실제값,예측 확률 값): 임계값 변화에 따른 평가지표를 반환\n",
    "- 반환값: 정밀도,재현율,임계 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d67cab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0723aad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision type -  <class 'numpy.ndarray'>\n",
      "recall type -  <class 'numpy.ndarray'>\n",
      "th type -  <class 'numpy.ndarray'>\n",
      "precision shape -  (149,)\n",
      "recall shape -  (149,)\n",
      "th shape - (148,)\n"
     ]
    }
   ],
   "source": [
    "# predict_proba_result = logistic_model.predict_proba(x_test)\n",
    "predict_proba_positive = predict_proba_result[:,1]\n",
    "precision, recall, th = precision_recall_curve(y_test, predict_proba_positive)\n",
    "print('Precision type - ',type(precision))\n",
    "print('recall type - ',type(recall))\n",
    "print('th type - ',type(th))\n",
    "print('precision shape - ', precision.shape)\n",
    "print('recall shape - ', recall.shape)\n",
    "print('th shape -',th.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "05f0b1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "\n",
    "plt.plot(th, precision[ 0 : th.shape[0]] , linestyle=\"--\", label=\"precision\")\n",
    "plt.plot(th, recall[ 0 : th.shape[0]]    , linestyle=\"-\" , label=\"recall\")\n",
    "\n",
    "plt.xlabel('threshold ratio')\n",
    "plt.ylabel('precision and recall value')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
